{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3095fde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 評価方法\n",
    "\n",
    "分類されていないデータを認識し、どれだけ正しくカテゴリごとに分類できるかを算出した「平均精度」の高さを競い合います。\n",
    "\n",
    "今回、活用するデータはLSWMD_25519となります。\n",
    "LSWMD_25519のFailureType項目が分類されていない状態のデータに対し、正しいFailureTypeカテゴリを分類するプログラムを作成し、その平均精度を算出します。\n",
    "平均精度とは、カテゴリごとに正しく分類できる精度を平均した値です。カテゴリごとに算出した精度（Aが正しく分類された数/Aのデータ数）を足し、カテゴリ数で割ります。\n",
    "\n",
    "公平な評価を実施するために、以下の制限を設けています。\n",
    "1. 外部パッケージをインストールするためのセルとsolution関数の中身のみを編集すること\n",
    "2. 校舎のiMac上で最後のセルの実行時間が15分未満であること　（%%timeitの出力結果を確認してください）\n",
    "\n",
    "※気になる点がある場合、Discordで気軽にお問合せください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8b0f68e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # https://numpy.org/ja/\n",
    "import pandas as pd # https://pandas.pydata.org/\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea86471-32fa-46c6-a005-ad6e1f7b7f72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "外部パッケージを使用する場合、以下の方法でインストールを実施してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be8d1cd-7df7-4b10-aa1a-e24677b50d78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 必要な外部パッケージは、以下の内容を編集しインストールしてください\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install scikit-learn\n",
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "#!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113ea05-433a-4c82-9cb1-2c8f834a0364",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "以下のsolution関数のみ編集してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda6ad7e-8e26-4208-bdbd-3fa37e79c82c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_map(map):\n",
    "    from PIL import Image\n",
    "\n",
    "    # リサイズ後のサイズを指定\n",
    "    resize_shape = (32, 32)\n",
    "    \n",
    "    # マップの次元を取得\n",
    "    len_y, len_x = map.shape\n",
    "\n",
    "    # マップの中心y座標とx座標を取得\n",
    "    y_add = len_y // 2 + len_y % 2\n",
    "    x_add = len_x // 2 + len_x % 2\n",
    "\n",
    "    # 0の値のインデックスを取得\n",
    "    y_indices, x_indices = np.where(map == 0)\n",
    "    # 他の位置に置換\n",
    "    map[y_indices, x_indices] = map[(y_indices + y_add) % len_y, (x_indices + x_add) % len_x]\n",
    "    # 0の値のインデックスを取得\n",
    "    y_indices, x_indices = np.where(map == 0)\n",
    "    # 0の値を置換\n",
    "    map[y_indices, x_indices] = 1\n",
    "\n",
    "    # リサイズし1を減算\n",
    "    map = Image.fromarray(map - 1.0)\n",
    "    # mapから1を減算してPILイメージを作成\n",
    "    resized_map = map.resize(resize_shape, Image.LANCZOS)\n",
    "\n",
    "    return np.asarray(resized_map)\n",
    "\n",
    "\n",
    "def preprocess_map(df, normalize_map):\n",
    "    # データの正規化\n",
    "    train_maps = np.array([normalize_map(x) for x in df['waferMap']])\n",
    "\n",
    "    # 1. 画像を水平方向に反転\n",
    "    flipped_horizontally = np.flip(train_maps, axis=2)\n",
    "    train_maps = np.concatenate((train_maps, flipped_horizontally), axis=0)\n",
    "\n",
    "    # # 2. 画像を垂直方向に反転\n",
    "    # flipped_vertically = np.flip(train_maps, axis=1)\n",
    "    # train_maps = np.concatenate((train_maps, flipped_vertically), axis=0)\n",
    "\n",
    "    # 3. 画像を90度回転\n",
    "    rotated_90 = np.rot90(train_maps, k=1, axes=(1, 2))\n",
    "    train_maps = np.concatenate((train_maps, rotated_90), axis=0)\n",
    "\n",
    "    # 4. 画像を180度回転\n",
    "    rotated_180 = np.rot90(train_maps, k=2, axes=(1, 2))\n",
    "    train_maps = np.concatenate((train_maps, rotated_180), axis=0)\n",
    "\n",
    "    # # 5. 画像を270度回転\n",
    "    # rotated_270 = np.rot90(train_maps, k=3, axes=(1, 2))\n",
    "    # train_maps = np.concatenate((train_maps, rotated_270), axis=0)\n",
    "\n",
    "    # データの形状を変更\n",
    "    train_maps = train_maps.reshape(train_maps.shape + (1,))\n",
    "\n",
    "    return train_maps\n",
    "\n",
    "\n",
    "def preprocess_size(df):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # dieSizeの正規化（0-1の間にスケーリング）\n",
    "    size = df['dieSize'].values\n",
    "    size = size / tf.reduce_max(size)\n",
    "\n",
    "    return size\n",
    "\n",
    "\n",
    "def create_hybrid_model(map_shape, num_classes):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, concatenate, Multiply\n",
    "    from tensorflow.keras.models import Model\n",
    "\n",
    "    # waferMap用の入力レイヤー\n",
    "    wafermap_input = Input(shape=map_shape, name='wafermap_input')\n",
    "    \n",
    "    # 畳み込みとプーリング層\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(wafermap_input)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "    x = Conv2D(40, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(48, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    wafermap_features = Flatten()(x)\n",
    "\n",
    "    # dieSize用の入力レイヤー\n",
    "    diesize_input = Input(shape=(1,), name='diesize_input')\n",
    "    diesize_dense = Dense(16, activation='relu')(diesize_input)\n",
    "\n",
    "    # 注意機構の適用\n",
    "    # `wafermap_features`の長さに合わせて`attention_probs`を変換\n",
    "    attention_dense = Dense(tf.keras.backend.int_shape(wafermap_features)[-1], activation='softmax', name='attention_vec')(diesize_dense)\n",
    "    attention_mul = Multiply()([wafermap_features, attention_dense])\n",
    "\n",
    "    # 特徴量の結合\n",
    "    combined_features = concatenate([attention_mul, diesize_dense])\n",
    "\n",
    "    # 密結合層とドロップアウト\n",
    "    x = Dense(320, activation='relu')(combined_features)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(80, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    # 出力層\n",
    "    outputs = Dense(num_classes)(x)\n",
    "\n",
    "    # モデルの定義\n",
    "    model = Model(inputs=[wafermap_input, diesize_input], outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_class_weights(train_labels):\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    # クラスの重みを計算\n",
    "    class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                         classes=np.unique(train_labels), \n",
    "                                         y=train_labels)\n",
    "    # class_weights[4] *= 1.5\n",
    "    # クラスの重みを辞書型に変換\n",
    "    return dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "def solution(x_test_df, train_df):\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "    import tensorflow as tf\n",
    "    #import keras_tuner as kt\n",
    "\n",
    "    # failureTypeのユニークな値を取得\n",
    "    failure_types = list(train_df['failureType'].unique())\n",
    "\n",
    "    test_maps = preprocess_map(x_test_df, normalize_map)\n",
    "    train_maps = preprocess_map(train_df, normalize_map)\n",
    "    test_sizes = np.repeat(preprocess_size(x_test_df), 8)\n",
    "    train_sizes = np.repeat(preprocess_size(train_df), 8)\n",
    "    train_labels = np.array([failure_types.index(x) for x in train_df['failureType']] * 8)\n",
    "\n",
    "    class_weights = calculate_class_weights(train_labels)\n",
    "\n",
    "    model = create_hybrid_model(train_maps[0].shape, len(failure_types))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit([train_maps, train_sizes], train_labels, epochs=5, class_weight=class_weights)\n",
    "\n",
    "    # tuner = kt.RandomSearch(\n",
    "    #     create_model,\n",
    "    #     objective='val_accuracy',\n",
    "    #     max_trials=10,\n",
    "    #     directory='tuner',\n",
    "    #     project_name='wafermap'\n",
    "    # )\n",
    "\n",
    "    # tuner.search(train_maps, train_labels, epochs=10, validation_split=0.1)\n",
    "\n",
    "    # best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    # model = tuner.hypermodel.build(best_hps)\n",
    "    # model.fit(train_maps, train_labels, epochs=10, class_weight=class_weights)\n",
    "\n",
    "    # 各予測結果の平均を計算\n",
    "    test_logits = np.mean(model.predict([test_maps, test_sizes]).reshape(-1, len(x_test_df['waferMap']), len(failure_types)), axis=0)\n",
    "    predictions = tf.nn.softmax(test_logits).numpy()\n",
    "    answer = [failure_types[x.argmax()] for x in predictions]\n",
    "\n",
    "    return pd.DataFrame({'failureType': answer}, index=x_test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c20f4-f775-4d9d-90c7-a3b583584edd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "solution関数は以下のように活用され、平均精度を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a31dda-7c8b-477e-9547-5c9db739f7f0",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5742/5742 [==============================] - 41s 7ms/step - loss: 2.1103 - accuracy: 0.3265\n",
      "638/638 [==============================] - 1s 2ms/step\n",
      "平均精度：12.50%\n",
      "45.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "# データのインポート\n",
    "df=pd.read_pickle(\"../input/LSWMD_25519.pkl\")\n",
    "\n",
    "# テスト用と学習用のデータを作成（テストする際は、random_stateの値などを編集してみてください）\n",
    "train_df, test_df = train_test_split(df, stratify=df['failureType'], test_size=0.10, random_state=42)\n",
    "\n",
    "y_test_df = test_df[['failureType']]\n",
    "x_test_df = test_df.drop(columns=['failureType'])\n",
    "\n",
    "# solution関数を実行\n",
    "user_result_df = solution(x_test_df, train_df)\n",
    "\n",
    "average_accuracy = 0\n",
    "# ユーザーの提出物のフォーマット確認\n",
    "if type(y_test_df) == type(user_result_df) and y_test_df.shape == user_result_df.shape:\n",
    "    # 平均精度の計算\n",
    "    accuracies = {}\n",
    "    for failure_type in df['failureType'].unique():\n",
    "        y_test_df_by_failure_type = y_test_df[y_test_df['failureType'] == failure_type]\n",
    "        user_result_df_by_failure_type = user_result_df[y_test_df['failureType'] == failure_type]\n",
    "        matching_rows = (y_test_df_by_failure_type == user_result_df_by_failure_type).all(axis=1).sum()\n",
    "        accuracies[failure_type] = (matching_rows/(len(y_test_df_by_failure_type)))\n",
    "    \n",
    "    average_accuracy = sum(accuracies.values())/len(accuracies)\n",
    "print(f\"平均精度：{average_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a499abb-7dc0-4b60-935f-cbc76941e21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
